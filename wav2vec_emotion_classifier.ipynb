{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suCYR0i0qOhL",
        "outputId": "f63911b1-70a2-4912-aa59-bf0e59993f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LCATFTgkqqcf"
      },
      "outputs": [],
      "source": [
        "!mkdir weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Zp2vHviWVpp"
      },
      "outputs": [],
      "source": [
        "# get max audio len from wavs in directory AudioWAV/\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn, optim\n",
        "import wandb\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YXn3OHDTWVpq"
      },
      "outputs": [],
      "source": [
        "# create full data df\n",
        "mypath = '/content/AudioWAV/'\n",
        "data = []\n",
        "for full_fname in os.listdir(mypath):\n",
        "    fname = full_fname[:-4]\n",
        "    tags = fname.split('_')\n",
        "    tags.append(mypath + full_fname)\n",
        "    data.append(tags)\n",
        "\n",
        "full_df = pd.DataFrame(data, columns=['subject_ID', 'sentence', 'emotion', 'intensity', 'path'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N7OWEG_CWVpq"
      },
      "outputs": [],
      "source": [
        "# split data into train, test, val based on unique subject IDs\n",
        "# also todo: try splitting on ID + some other demographic feature (e.g. sex, race, country etc.)\n",
        "\n",
        "unique_subjects = full_df['subject_ID'].unique()\n",
        "\n",
        "# Step 2: Shuffle and split subjects into train, test, and validation sets\n",
        "train_subjects, temp_subjects = train_test_split(unique_subjects, test_size=0.3, random_state=42)\n",
        "test_subjects, val_subjects = train_test_split(temp_subjects, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 3: Create DataFrames for each split based on the subject IDs\n",
        "train_df = full_df[full_df['subject_ID'].isin(train_subjects)]\n",
        "test_df = full_df[full_df['subject_ID'].isin(test_subjects)]\n",
        "val_df = full_df[full_df['subject_ID'].isin(val_subjects)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyb-zCETWVpq",
        "outputId": "5e8c22ec-6605-4539-d125-c1bf5eaca618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set samples: 5147\n",
            "Test set samples: 1147\n",
            "Validation set samples: 1148\n",
            "\n",
            "\n",
            "emotion\n",
            "DIS    0.170779\n",
            "SAD    0.170779\n",
            "FEA    0.170779\n",
            "ANG    0.170779\n",
            "HAP    0.170779\n",
            "NEU    0.146105\n",
            "Name: count, dtype: float64\n",
            "emotion\n",
            "DIS    0.170881\n",
            "FEA    0.170881\n",
            "HAP    0.170881\n",
            "SAD    0.170881\n",
            "ANG    0.170881\n",
            "NEU    0.145597\n",
            "Name: count, dtype: float64\n",
            "emotion\n",
            "HAP    0.170732\n",
            "ANG    0.170732\n",
            "FEA    0.170732\n",
            "SAD    0.170732\n",
            "DIS    0.170732\n",
            "NEU    0.146341\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# some statistics:\n",
        "\n",
        "# Print the number of samples in each split to verify\n",
        "print(\"Train set samples:\", len(train_df))\n",
        "print(\"Test set samples:\", len(test_df))\n",
        "print(\"Validation set samples:\", len(val_df))\n",
        "print('\\n')\n",
        "\n",
        "# get percentage of different emotions in train set\n",
        "emotion_counts = train_df['emotion'].value_counts()\n",
        "emotion_counts = emotion_counts / emotion_counts.sum()\n",
        "print(emotion_counts)\n",
        "\n",
        "# same for test and val\n",
        "emotion_counts = test_df['emotion'].value_counts()\n",
        "emotion_counts = emotion_counts / emotion_counts.sum()\n",
        "print(emotion_counts)\n",
        "\n",
        "emotion_counts = val_df['emotion'].value_counts()\n",
        "emotion_counts = emotion_counts / emotion_counts.sum()\n",
        "print(emotion_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuiqEewYWVpq"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gThfw2vFWVpr"
      },
      "outputs": [],
      "source": [
        "class CustomAudioDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels_metainfo, max_len, target='emotion_label', processor_name=\"facebook/wav2vec2-large-960h-lv60-self\", sampling_rate=16000, fp16=True):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = list(labels_metainfo[target])\n",
        "        self.max_len = max_len\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.processor = Wav2Vec2Processor.from_pretrained(processor_name)\n",
        "        self.fp16 = fp16\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load audio file using librosa\n",
        "        audio, sr = librosa.load(self.file_paths[idx], sr=self.sampling_rate)\n",
        "        audio = torch.from_numpy(audio).float()\n",
        "\n",
        "        # Pad/Truncate the audio to max_len\n",
        "        if audio.shape[0] > self.max_len:\n",
        "            audio = audio[:self.max_len]\n",
        "        else:\n",
        "            pad_len = self.max_len - audio.shape[0]\n",
        "            audio = torch.nn.functional.pad(audio, (0, pad_len))\n",
        "\n",
        "        # Use processor to preprocess the audio\n",
        "        inputs = self.processor(audio, sampling_rate=self.sampling_rate, return_tensors=\"pt\", padding=True)\n",
        "        input_values = inputs.input_values.squeeze(0)  # Remove batch dimension\n",
        "        if self.fp16:\n",
        "          input_values = input_values.to(torch.float16)\n",
        "\n",
        "        # Get the corresponding label\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return input_values, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f3z4eM8WVpr"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IKQ1Tv5ZWVpr"
      },
      "outputs": [],
      "source": [
        "class AudioClassifier(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(AudioClassifier, self).__init__()\n",
        "        self.config = config\n",
        "        if config['fp16']:\n",
        "          self.body = Wav2Vec2Model.from_pretrained(config['model_name'], torch_dtype=torch.float16)\n",
        "        else:\n",
        "          self.body = Wav2Vec2Model.from_pretrained(config['model_name'])\n",
        "        if self.config['freeze_cnn']:\n",
        "            for param in self.body.feature_extractor.parameters():\n",
        "                param.requires_grad = False\n",
        "        self.avg_pool = nn.AvgPool1d(kernel_size=config['in_seq_len'])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(config['in_dim'], config['hidden_dim']),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(config['hidden_dim']),\n",
        "            nn.Dropout(p=config['dropout']),\n",
        "            nn.Linear(config['hidden_dim'], config['num_classes'])\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.body(x).last_hidden_state\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.squeeze(2)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ClassifierModel(pl.LightningModule):\n",
        "    def __init__(self, config):\n",
        "        super(ClassifierModel, self).__init__()\n",
        "        self.config = config\n",
        "        self.model = AudioClassifier(config)\n",
        "\n",
        "        if self.config['kaiming_init']:\n",
        "            self.model.classifier.apply(self.init_weights)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.learning_rate = config['learning_rate']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.val_progress = []\n",
        "        self.train_progress = []\n",
        "        self.test_progress = []\n",
        "        self.val_loss = 0.0\n",
        "        self.train_loss = 0.0\n",
        "        self.best_acc = 0\n",
        "        self.best_f1 = 0\n",
        "        self.best_epoch = 0\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # initialize Adam optimizer and scheduler with warmup\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.config['weight_decay'])\n",
        "        if self.config['scheduler'] == 'cycle':\n",
        "            self.scheduler = optim.lr_scheduler.CyclicLR(self.optimizer, base_lr=self.config['min_lr'], max_lr=self.config['max_lr'], step_size_up=self.config['step_size'], cycle_momentum=False)\n",
        "            return [self.optimizer], [self.scheduler]\n",
        "        elif self.config['scheduler'] == 'step':\n",
        "            self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=self.config['step_size'], gamma=self.config['gamma'])\n",
        "        else:\n",
        "            return [self.optimizer]\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_uniform_(m.weight, nonlinearity=self.config['init_nonlinearity'])\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        probs = self(x)\n",
        "        if self.config['fp16']:\n",
        "          probs = probs.float()\n",
        "        loss = self.criterion(probs, y)\n",
        "        self.train_loss += loss.item()\n",
        "        self.log('train_loss', loss)\n",
        "        probs = nn.functional.softmax(probs, dim=1)\n",
        "        self.train_progress.append((probs, y))\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        probs = self(x)\n",
        "        if self.config['fp16']:\n",
        "          probs = probs.float()\n",
        "        loss = self.criterion(probs, y)\n",
        "        self.val_loss += loss.item()\n",
        "        self.log('val_loss', loss)\n",
        "        # save predictions and true labels to calculate accuracy on full validation after epoch end\n",
        "        probs = nn.functional.softmax(probs, dim=1)\n",
        "        self.val_progress.append((probs, y))\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # calculate accuracy on full validation set\n",
        "        preds = torch.cat([pred for pred, y in self.val_progress], dim=0)\n",
        "        y = torch.cat([y for pred, y in self.val_progress], dim=0)\n",
        "        preds = torch.argmax(preds, dim=1)\n",
        "        acc = accuracy_score(y.cpu(), preds.cpu())\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y.cpu(), preds.cpu(), average='macro')\n",
        "\n",
        "        if len(self.train_progress) > 0:\n",
        "            train_preds = torch.cat([pred for pred, y in self.train_progress], dim=0)\n",
        "            train_y = torch.cat([y for pred, y in self.train_progress], dim=0)\n",
        "            train_preds = torch.argmax(train_preds, dim=1)\n",
        "            train_acc = accuracy_score(train_y.cpu(), train_preds.cpu())\n",
        "        else:\n",
        "            train_acc = 0.0\n",
        "\n",
        "        # log metrics based on configuration settings\n",
        "        if self.config['log'] in ['wandb', 'all']:\n",
        "            log_dict = {'train_loss': self.train_loss, 'train acc': train_acc, 'val_loss': self.val_loss, 'val_acc': acc, 'val_precision': precision, 'val_recall': recall, 'val_f1': f1}\n",
        "            if self.config['scheduler'] == 'step':\n",
        "                log_dict['lr'] = self.scheduler.get_last_lr()[0]\n",
        "            wandb.log(log_dict)\n",
        "        if self.config['log'] in ['stdout', 'all']:\n",
        "            print(f'Training loss: {self.train_loss}')\n",
        "            print(f'Validation accuracy: {acc}')\n",
        "            print(f'Validation loss: {self.val_loss}')\n",
        "            print(f'Validation precision: {precision}')\n",
        "            print(f'Validation recall: {recall}')\n",
        "            print(f'Validation f1: {f1}')\n",
        "            if self.config['scheduler'] == 'step':\n",
        "                print(f'Learning rate: {self.scheduler.get_last_lr()[0]}')\n",
        "\n",
        "        # saved best model, based on chosen target metric if save_best is set to True\n",
        "        if self.config['save_best']:\n",
        "            if self.config['target_metric'] == 'accuracy' and acc >= self.best_acc:\n",
        "                self.best_acc = acc\n",
        "                self.save_checkpoint('/content/weights/'+self.config['run_name']+'.pth')\n",
        "                self.best_epoch = self.current_epoch\n",
        "                self.best_f1 = f1\n",
        "            elif self.config['target_metric'] == 'f1' and f1 >= self.best_f1:\n",
        "                self.best_f1 = f1\n",
        "                self.save_checkpoint('/content/weights/'+self.config['run_name']+'.pth')\n",
        "                self.best_epoch = self.current_epoch\n",
        "                self.best_acc = acc\n",
        "\n",
        "        # reset variables for next epoch\n",
        "        self.val_progress = []\n",
        "        self.val_loss = 0.0\n",
        "        self.train_loss = 0.0\n",
        "        self.train_progress = []\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        probs = self(x)\n",
        "        if self.config['fp16']:\n",
        "          probs = probs.float()\n",
        "        loss = self.criterion(probs, y)\n",
        "        self.log('test_loss', loss)\n",
        "        probs = nn.functional.softmax(probs, dim=1)\n",
        "        self.test_progress.append((probs, y))\n",
        "        return loss\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        # calculate accuracy on full test set\n",
        "        preds = torch.cat([pred for pred, y in self.test_progress], dim=0)\n",
        "        y = torch.cat([y for pred, y in self.test_progress], dim=0)\n",
        "        preds = torch.argmax(preds, dim=1)\n",
        "        acc = accuracy_score(y.cpu(), preds.cpu())\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y.cpu(), preds.cpu(), average='macro')\n",
        "\n",
        "        # log metrics based on configuration settings\n",
        "        if self.config['log'] in ['wandb', 'all']:\n",
        "            wandb.log({'test_acc': acc, 'test_precision': precision, 'test_recall': recall, 'test_f1': f1})\n",
        "        if self.config['log'] in ['stdout', 'all']:\n",
        "            print(f'Test accuracy: {acc}')\n",
        "            print(f'Test precision: {precision}')\n",
        "            print(f'Test recall: {recall}')\n",
        "            print(f'Test f1: {f1}')\n",
        "\n",
        "        # reset variables for next epoch\n",
        "        self.test_progress = []\n",
        "\n",
        "\n",
        "    def save_checkpoint(self, path):\n",
        "        \"\"\"\n",
        "        Saves a checkpoint, optimizer, and scheduler from a specified path.\n",
        "        :param path: The file path from which to load the checkpoint.\n",
        "        \"\"\"\n",
        "        model_state = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict()\n",
        "        }\n",
        "        if self.config['scheduler'] is not None:\n",
        "            model_state['scheduler_state_dict'] = self.scheduler.state_dict()\n",
        "        torch.save(model_state, path)\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        \"\"\"\n",
        "        Loads a checkpoint, optimizer, and scheduler from a specified path.\n",
        "        :param path: The file path from which to load the checkpoint.\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(path)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        if self.config['scheduler'] is not None:\n",
        "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4P6x8AnWVpr"
      },
      "source": [
        "# misc processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E4hCtWGdWVpr"
      },
      "outputs": [],
      "source": [
        "emotions_label_encoder = {\n",
        "    'ANG':0,\n",
        "    'DIS':1,\n",
        "    'FEA':2,\n",
        "    'HAP':3,\n",
        "    'NEU':4,\n",
        "    'SAD':5\n",
        "}\n",
        "emotions_label_decoder = {v: k for k, v in emotions_label_encoder.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pdyc8kOzWVpr"
      },
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "\n",
        "mypath = '/content/AudioWAV/'\n",
        "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "for file in onlyfiles:\n",
        "    audio, sr = librosa.load(mypath + file, sr=16000)\n",
        "    if audio.shape[0] > max_len:\n",
        "        max_len = audio.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz49dyvIWVpr",
        "outputId": "c10b9623-b37b-424d-9f07-2a671cb78fc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-5d1ff4ea6cd1>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['emotion_label'] = train_df['emotion'].map(emotions_label_encoder)\n",
            "<ipython-input-9-5d1ff4ea6cd1>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df['emotion_label'] = test_df['emotion'].map(emotions_label_encoder)\n",
            "<ipython-input-9-5d1ff4ea6cd1>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df['emotion_label'] = val_df['emotion'].map(emotions_label_encoder)\n"
          ]
        }
      ],
      "source": [
        "train_df['emotion_label'] = train_df['emotion'].map(emotions_label_encoder)\n",
        "test_df['emotion_label'] = test_df['emotion'].map(emotions_label_encoder)\n",
        "val_df['emotion_label'] = val_df['emotion'].map(emotions_label_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwVZr1AgWVpr"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jrGSo8t0WVpr"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"processor_name\": \"audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\",\n",
        "    \"model_name\": \"audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\",\n",
        "    \"sampling_rate\": 16000,\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 5e-5,\n",
        "    'kaiming_init': True,\n",
        "    'init_nonlinearity': 'leaky_relu',\n",
        "    'freeze_cnn': True,\n",
        "    'in_seq_len': 250,\n",
        "    'in_dim': 1024,\n",
        "    'hidden_dim': 512,\n",
        "    'num_classes': 6,\n",
        "    'fp16': False,\n",
        "    'wandb_api_key': 'your-key',\n",
        "    'scheduler': None,\n",
        "    'log': 'wandb',\n",
        "    'save_best': True,\n",
        "    'target_metric': 'accuracy',\n",
        "    'dropout': 0.4,\n",
        "    'weight_decay': 1e-4,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "f_8Xxe0eWVpr",
        "outputId": "87fc30f6-ef72-473a-8c8b-9ac374020c3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtimothy-senchenko\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240610_221923-ut9056u7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/timothy-senchenko/audio-emotion-classification/runs/ut9056u7' target=\"_blank\">driven-monkey-22</a></strong> to <a href='https://wandb.ai/timothy-senchenko/audio-emotion-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/timothy-senchenko/audio-emotion-classification' target=\"_blank\">https://wandb.ai/timothy-senchenko/audio-emotion-classification</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/timothy-senchenko/audio-emotion-classification/runs/ut9056u7' target=\"_blank\">https://wandb.ai/timothy-senchenko/audio-emotion-classification/runs/ut9056u7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ],
      "source": [
        "wandb.init(project='audio-emotion-classification', config=config)\n",
        "wandb.login(key = config['wandb_api_key'])\n",
        "config['run_name'] = wandb.run.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJSmYmkoWVps",
        "outputId": "77addf7c-ee2b-48f8-b0fb-a3cd03c144a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_set = CustomAudioDataset(list(train_df['path']), train_df.drop('path', axis=1), max_len, processor_name=config['processor_name'], fp16=config['fp16'])\n",
        "val_set = CustomAudioDataset(list(val_df['path']), val_df.drop('path', axis=1), max_len, processor_name=config['processor_name'], fp16=config['fp16'])\n",
        "test_set = CustomAudioDataset(list(test_df['path']), test_df.drop('path', axis=1), max_len, processor_name=config['processor_name'], fp16=config['fp16'])\n",
        "\n",
        "# Create DataLoader\n",
        "trainloader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(val_set, batch_size=config['batch_size'], shuffle=False)\n",
        "testloader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3l109H3WVps",
        "outputId": "724406e7-b10f-4f9f-8859-4a0182c1ef56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "cls = ClassifierModel(config)\n",
        "if config['fp16']:\n",
        "  cls = cls.to(torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTxdTISnWVps",
        "outputId": "0d0f7706-ffbc-4afb-eef7-b77aad93e870"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "if config['fp16']:\n",
        "  trainer = pl.Trainer(max_epochs=10, check_val_every_n_epoch=1, precision='16-true')\n",
        "else:\n",
        "  trainer = pl.Trainer(max_epochs=10, check_val_every_n_epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G8i3pBOWVps"
      },
      "outputs": [],
      "source": [
        "trainer.fit(cls, trainloader, valloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.-1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
